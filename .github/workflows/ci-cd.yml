name: Data & AI Pipeline CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.9'
  AWS_REGION: us-east-1

jobs:
  lint-and-test:
    name: Lint and Test
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov black flake8 pylint
    
    - name: Run Black (code formatter check)
      run: |
        black --check src/ tests/
      continue-on-error: true
    
    - name: Run Flake8 (linting)
      run: |
        flake8 src/ tests/ --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 src/ tests/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
      continue-on-error: true
    
    - name: Run Pylint
      run: |
        pylint src/ --exit-zero --max-line-length=127
      continue-on-error: true
    
    - name: Run unit tests with coverage
      run: |
        pytest tests/ -v --cov=src --cov-report=xml --cov-report=html --cov-report=term
    
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella
      continue-on-error: true
  
  data-contract-validation:
    name: Data Contract Validation
    runs-on: ubuntu-latest
    needs: lint-and-test
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run schema validation tests
      run: |
        pytest tests/test_data_contracts.py -v
  
  build-and-package:
    name: Build and Package
    runs-on: ubuntu-latest
    needs: [lint-and-test, data-contract-validation]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Build Python package
      run: |
        python -m pip install --upgrade pip setuptools wheel
        python setup.py sdist bdist_wheel
    
    - name: Create deployment archive
      run: |
        tar -czf pipeline-deployment.tar.gz src/ config/ requirements.txt setup.py
    
    - name: Upload artifact
      uses: actions/upload-artifact@v3
      with:
        name: pipeline-package
        path: pipeline-deployment.tar.gz
  
  deploy-to-aws:
    name: Deploy to AWS
    runs-on: ubuntu-latest
    needs: build-and-package
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v3
    
    - name: Download artifact
      uses: actions/download-artifact@v3
      with:
        name: pipeline-package
    
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
    
    - name: Upload to S3
      run: |
        aws s3 cp pipeline-deployment.tar.gz s3://${{ secrets.S3_BUCKET_NAME }}/deployments/pipeline-deployment-${{ github.sha }}.tar.gz
        aws s3 cp pipeline-deployment.tar.gz s3://${{ secrets.S3_BUCKET_NAME }}/deployments/pipeline-deployment-latest.tar.gz
    
    - name: Upload source data to S3
      run: |
        aws s3 sync data/source1/ s3://${{ secrets.S3_BUCKET_NAME }}/data/source1/ --delete
        aws s3 sync data/source2/ s3://${{ secrets.S3_BUCKET_NAME }}/data/source2/ --delete
    
    - name: Trigger EMR Step (Optional)
      run: |
        echo "Deployment package uploaded to S3"
        echo "To run on EMR, submit a step with the uploaded package"
        # Uncomment below to auto-trigger EMR job
        # aws emr add-steps --cluster-id ${{ secrets.EMR_CLUSTER_ID }} \
        #   --steps Type=Spark,Name="Data-AI-Pipeline",ActionOnFailure=CONTINUE,Args=[--deploy-mode,cluster,--master,yarn,s3://${{ secrets.S3_BUCKET_NAME }}/deployments/pipeline-deployment-latest.tar.gz]
  
  notify:
    name: Notify
    runs-on: ubuntu-latest
    needs: [deploy-to-aws]
    if: always()
    
    steps:
    - name: Send notification
      run: |
        echo "Pipeline status: ${{ job.status }}"
        # Add Slack/Email notification here if needed
